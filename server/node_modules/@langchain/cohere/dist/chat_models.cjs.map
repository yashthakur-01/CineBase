{"version":3,"file":"chat_models.cjs","names":["observations: MessageContent","documents: Array<Record<string, any>>","observationsList: Array<Record<string, any>>","message: BaseMessage","toolResults: ToolResult[]","role: MessageType","content: MessageContent","message","tool: any","jsonSchema: Record<string, any>","parameterDefinitionsFinal: Record<string, any>","tools: ChatCohereCallOptions[\"tools\"]","isOpenAITool","isLangChainTool","BaseChatModel","fields?: ChatCohereInput","getCohereClient","options: this[\"ParsedCallOptions\"]","tools: ChatCohereToolType[]","kwargs?: Partial<CallOptions>","messages: BaseMessage[]","messageStr: string","tempToolResults: {\n      call: Cohere.ToolCall;\n      outputs: any;\n    }[]","req: Cohere.ChatRequest","currentChatTurnMessages: BaseMessage[]","toolResults: Array<{\n      call: Cohere.ToolCall;\n      outputs: ReturnType<typeof convertToDocuments>;\n    }>","toolMessageIndex: number","toolResults: Array<{ call: Cohere.ToolCall; outputs: any }>","toolCalls: Cohere.ToolCall[] | null","toolCalls: Record<string, any>[]","runManager?: CallbackManagerForLLMRun","tokenUsage: TokenUsage","finalChunks: Record<number, ChatGenerationChunk>","generations","response: Cohere.NonStreamedChatResponse","response","e: any","generationInfo: Record<string, unknown>","toolCalls: ToolCall[]","generations: ChatGeneration[]","AIMessage","stream","ChatGenerationChunk","AIMessageChunk","chunkGenerationInfo: Record<string, any>","toolCallChunks: ToolCallChunk[]","toolCall: any"],"sources":["../src/chat_models.ts"],"sourcesContent":["/* eslint-disable @typescript-eslint/no-explicit-any */\nimport { Cohere, CohereClient } from \"cohere-ai\";\nimport { ToolResult } from \"cohere-ai/api/index.js\";\nimport {\n  AIMessage,\n  type BaseMessage,\n  isAIMessage,\n  MessageContent,\n  MessageType,\n} from \"@langchain/core/messages\";\nimport {\n  BaseLanguageModelInput,\n  isOpenAITool,\n} from \"@langchain/core/language_models/base\";\nimport { isLangChainTool } from \"@langchain/core/utils/function_calling\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport {\n  BaseChatModel,\n  BaseChatModelCallOptions,\n  type BaseChatModelParams,\n  BindToolsInput,\n  LangSmithParams,\n} from \"@langchain/core/language_models/chat_models\";\nimport {\n  ChatGeneration,\n  ChatGenerationChunk,\n  ChatResult,\n} from \"@langchain/core/outputs\";\nimport { AIMessageChunk } from \"@langchain/core/messages\";\nimport { NewTokenIndices } from \"@langchain/core/callbacks/base\";\nimport {\n  ToolCall,\n  ToolCallChunk,\n  ToolMessage,\n} from \"@langchain/core/messages/tool\";\nimport * as uuid from \"uuid\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { isInteropZodSchema } from \"@langchain/core/utils/types\";\nimport { toJsonSchema } from \"@langchain/core/utils/json_schema\";\nimport { CohereClientOptions, getCohereClient } from \"./client.js\";\n\ntype ChatCohereToolType = BindToolsInput | Cohere.Tool;\n\n/**\n * Input interface for ChatCohere\n */\nexport interface BaseChatCohereInput extends BaseChatModelParams {\n  /**\n   * The API key to use.\n   * @default {process.env.COHERE_API_KEY}\n   */\n  apiKey?: string;\n  /**\n   * The name of the model to use.\n   * @default {\"command\"}\n   */\n  model?: string;\n  /**\n   * What sampling temperature to use, between 0.0 and 2.0.\n   * Higher values like 0.8 will make the output more random,\n   * while lower values like 0.2 will make it more focused\n   * and deterministic.\n   * @default {0.3}\n   */\n  temperature?: number;\n  /**\n   * Whether or not to stream the response.\n   * @default {false}\n   */\n  streaming?: boolean;\n  /**\n   * Whether or not to include token usage when streaming.\n   * This will include an extra chunk at the end of the stream\n   * with `eventType: \"stream-end\"` and the token usage in\n   * `usage_metadata`.\n   * @default {true}\n   */\n  streamUsage?: boolean;\n}\n\nexport type ChatCohereInput = BaseChatCohereInput & CohereClientOptions;\n\ninterface TokenUsage {\n  completionTokens?: number;\n  promptTokens?: number;\n  totalTokens?: number;\n}\n\nexport interface ChatCohereCallOptions\n  extends BaseChatModelCallOptions,\n    Partial<Omit<Cohere.ChatRequest, \"message\" | \"tools\">>,\n    Partial<Omit<Cohere.ChatStreamRequest, \"message\" | \"tools\">>,\n    Pick<ChatCohereInput, \"streamUsage\"> {\n  tools?: ChatCohereToolType[];\n}\n\nfunction convertToDocuments(\n  observations: MessageContent\n): Array<Record<string, any>> {\n  /** Converts observations into a 'document' dict */\n  const documents: Array<Record<string, any>> = [];\n  let observationsList: Array<Record<string, any>> = [];\n\n  if (typeof observations === \"string\") {\n    // strings are turned into a key/value pair and a key of 'output' is added.\n    observationsList = [{ output: observations }];\n  } else if (\n    // eslint-disable-next-line no-instanceof/no-instanceof\n    observations instanceof Map ||\n    (typeof observations === \"object\" &&\n      observations !== null &&\n      !Array.isArray(observations))\n  ) {\n    // single mappings are transformed into a list to simplify the rest of the code.\n    observationsList = [observations];\n  } else if (!Array.isArray(observations)) {\n    // all other types are turned into a key/value pair within a list\n    observationsList = [{ output: observations }];\n  }\n\n  for (let doc of observationsList) {\n    // eslint-disable-next-line no-instanceof/no-instanceof\n    if (!(doc instanceof Map) && (typeof doc !== \"object\" || doc === null)) {\n      // types that aren't Mapping are turned into a key/value pair.\n      doc = { output: doc };\n    }\n    documents.push(doc);\n  }\n\n  return documents;\n}\n\nfunction convertMessageToCohereMessage(\n  message: BaseMessage,\n  toolResults: ToolResult[]\n): Cohere.Message {\n  const getRole = (role: MessageType) => {\n    switch (role) {\n      case \"system\":\n        return \"SYSTEM\";\n      case \"human\":\n        return \"USER\";\n      case \"ai\":\n        return \"CHATBOT\";\n      case \"tool\":\n        return \"TOOL\";\n      default:\n        throw new Error(\n          `Unknown message type: '${role}'. Accepted types: 'human', 'ai', 'system', 'tool'`\n        );\n    }\n  };\n\n  const getContent = (content: MessageContent): string => {\n    if (typeof content === \"string\") {\n      return content;\n    }\n    throw new Error(\n      `ChatCohere does not support non text message content. Received: ${JSON.stringify(\n        content,\n        null,\n        2\n      )}`\n    );\n  };\n\n  const getToolCall = (message: BaseMessage): Cohere.ToolCall[] => {\n    if (isAIMessage(message) && message.tool_calls) {\n      return message.tool_calls.map((toolCall) => ({\n        name: toolCall.name,\n        parameters: toolCall.args,\n      }));\n    }\n    return [];\n  };\n  if (message._getType().toLowerCase() === \"ai\") {\n    return {\n      role: getRole(message._getType()),\n      message: getContent(message.content),\n      toolCalls: getToolCall(message),\n    };\n  } else if (message._getType().toLowerCase() === \"tool\") {\n    return {\n      role: getRole(message._getType()),\n      message: getContent(message.content),\n      toolResults,\n    };\n  } else if (\n    message._getType().toLowerCase() === \"human\" ||\n    message._getType().toLowerCase() === \"system\"\n  ) {\n    return {\n      role: getRole(message._getType()),\n      message: getContent(message.content),\n    };\n  } else {\n    throw new Error(\n      \"Got unknown message type. Supported types are AIMessage, ToolMessage, HumanMessage, and SystemMessage\"\n    );\n  }\n}\n\nfunction isCohereTool(tool: any): tool is Cohere.Tool {\n  return (\n    \"name\" in tool && \"description\" in tool && \"parameterDefinitions\" in tool\n  );\n}\n\nfunction isToolMessage(message: BaseMessage): message is ToolMessage {\n  return message._getType() === \"tool\";\n}\n\nfunction _convertJsonSchemaToCohereTool(jsonSchema: Record<string, any>) {\n  const parameterDefinitionsProperties =\n    \"properties\" in jsonSchema ? jsonSchema.properties : {};\n  let parameterDefinitionsRequired =\n    \"required\" in jsonSchema ? jsonSchema.required : [];\n\n  const parameterDefinitionsFinal: Record<string, any> = {};\n\n  // Iterate through all properties\n  Object.keys(parameterDefinitionsProperties).forEach((propertyName) => {\n    // Create the property in the new object\n    parameterDefinitionsFinal[propertyName] =\n      parameterDefinitionsProperties[propertyName];\n    // Set the required property based on the 'required' array\n    if (parameterDefinitionsRequired === undefined) {\n      parameterDefinitionsRequired = [];\n    }\n    parameterDefinitionsFinal[propertyName].required =\n      parameterDefinitionsRequired.includes(propertyName);\n  });\n  return parameterDefinitionsFinal;\n}\n\nfunction _formatToolsToCohere(\n  tools: ChatCohereCallOptions[\"tools\"]\n): Cohere.Tool[] | undefined {\n  if (!tools) {\n    return undefined;\n  } else if (tools.every(isCohereTool)) {\n    return tools;\n  } else if (tools.every(isOpenAITool)) {\n    return tools.map((tool) => {\n      return {\n        name: tool.function.name,\n        description: tool.function.description ?? \"\",\n        parameterDefinitions: _convertJsonSchemaToCohereTool(\n          tool.function.parameters\n        ),\n      };\n    });\n  } else if (tools.every(isLangChainTool)) {\n    return tools.map((tool) => {\n      const parameterDefinitionsFromZod = isInteropZodSchema(tool.schema)\n        ? toJsonSchema(tool.schema)\n        : tool.schema;\n      return {\n        name: tool.name,\n        description: tool.description ?? \"\",\n        parameterDefinitions: _convertJsonSchemaToCohereTool(\n          parameterDefinitionsFromZod\n        ),\n      };\n    });\n  } else {\n    throw new Error(\n      `Can not pass in a mix of tool schema types to ChatCohere.`\n    );\n  }\n}\n\n/**\n * Integration for Cohere chat models.\n *\n * Setup:\n * Install `@langchain/cohere` and set a environment variable called `COHERE_API_KEY`.\n *\n * ```bash\n * npm install @langchain/cohere\n * export COHERE_API_KEY=\"your-api-key\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/langchain_cohere.ChatCohere.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/langchain_cohere.ChatCohereCallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n *   tools: [...],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     stop: [\"\\n\"],\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatCohere } from '@langchain/cohere';\n *\n * const llm = new ChatCohere({\n *   model: \"command-r-plus\",\n *   temperature: 0,\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"content\": \"\\\"J'adore programmer.\\\"\",\n *   \"additional_kwargs\": {\n *     ...\n *   },\n *   \"response_metadata\": {\n *     \"estimatedTokenUsage\": {\n *       \"completionTokens\": 6,\n *       \"promptTokens\": 75,\n *       \"totalTokens\": 81\n *     },\n *     \"response_id\": \"54cebd43-737f-458b-bff4-01b220eaf373\",\n *     \"generationId\": \"48a567da-0f88-4606-bba6-becbeee464bd\",\n *     \"chatHistory\": [\n *       {\n *         \"role\": \"USER\",\n *         \"message\": \"Translate \\\"I love programming\\\" into French.\"\n *       },\n *       {\n *         \"role\": \"CHATBOT\",\n *         \"message\": \"\\\"J'adore programmer.\\\"\"\n *       }\n *     ],\n *     \"finishReason\": \"COMPLETE\",\n *     \"meta\": {\n *       \"apiVersion\": {\n *         \"version\": \"1\"\n *       },\n *       \"billedUnits\": {\n *         \"inputTokens\": 9,\n *         \"outputTokens\": 6\n *       },\n *       \"tokens\": {\n *         \"inputTokens\": 75,\n *         \"outputTokens\": 6\n *       }\n *     }\n *   },\n *   \"tool_calls\": [],\n *   \"invalid_tool_calls\": [],\n *   \"usage_metadata\": {\n *     \"input_tokens\": 75,\n *     \"output_tokens\": 6,\n *     \"total_tokens\": 81\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"additional_kwargs\": {\n *     \"eventType\": \"stream-start\",\n *     \"is_finished\": false,\n *     \"generationId\": \"d62c8989-8af5-4357-af79-4ea8e6eb2baa\"\n *   },\n *   \"response_metadata\": {\n *     \"eventType\": \"stream-start\",\n *     \"is_finished\": false,\n *     \"generationId\": \"d62c8989-8af5-4357-af79-4ea8e6eb2baa\"\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \"\\\"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \"J\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \"'\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \"adore\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \" programmer\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \".\\\"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"additional_kwargs\": {\n *     \"eventType\": \"stream-end\"\n *   },\n *   \"response_metadata\": {\n *     \"eventType\": \"stream-end\",\n *     \"response_id\": \"687f94a6-13b7-4c2c-98be-9ca5573c722f\",\n *     \"text\": \"\\\"J'adore programmer.\\\"\",\n *     \"generationId\": \"d62c8989-8af5-4357-af79-4ea8e6eb2baa\",\n *     \"chatHistory\": [\n *       {\n *         \"role\": \"USER\",\n *         \"message\": \"Translate \\\"I love programming\\\" into French.\"\n *       },\n *       {\n *         \"role\": \"CHATBOT\",\n *         \"message\": \"\\\"J'adore programmer.\\\"\"\n *       }\n *     ],\n *     \"finishReason\": \"COMPLETE\",\n *     \"meta\": {\n *       \"apiVersion\": {\n *         \"version\": \"1\"\n *       },\n *       \"billedUnits\": {\n *         \"inputTokens\": 9,\n *         \"outputTokens\": 6\n *       },\n *       \"tokens\": {\n *         \"inputTokens\": 75,\n *         \"outputTokens\": 6\n *       }\n *     }\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": [],\n *   \"usage_metadata\": {\n *     \"input_tokens\": 75,\n *     \"output_tokens\": 6,\n *     \"total_tokens\": 81\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"content\": \"\\\"J'adore programmer.\\\"\",\n *   \"additional_kwargs\": {\n *     ...\n *   },\n *   \"response_metadata\": {\n *     \"is_finished\": false,\n *     \"generationId\": \"303e0215-96f4-4da5-8c2a-10da3840afce303e0215-96f4-4da5-8c2a-10da3840afce\",\n *     \"response_id\": \"6a8cb7ef-f1b9-44f6-a1df-67aa506d3f0f\",\n *     \"text\": \"\\\"J'adore programmer.\\\"\",\n *     \"chatHistory\": [\n *       {\n *         \"role\": \"USER\",\n *         \"message\": \"Translate \\\"I love programming\\\" into French.\"\n *       },\n *       {\n *         \"role\": \"CHATBOT\",\n *         \"message\": \"\\\"J'adore programmer.\\\"\"\n *       }\n *     ],\n *     \"finishReason\": \"COMPLETE\",\n *     \"meta\": {\n *       \"apiVersion\": {\n *         \"version\": \"1\"\n *       },\n *       \"billedUnits\": {\n *         \"inputTokens\": 9,\n *         \"outputTokens\": 6\n *       },\n *       \"tokens\": {\n *         \"inputTokens\": 75,\n *         \"outputTokens\": 6\n *       }\n *     }\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": [],\n *   \"usage_metadata\": {\n *     \"input_tokens\": 75,\n *     \"output_tokens\": 6,\n *     \"total_tokens\": 81\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools([GetWeather, GetPopulation]);\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'LA' },\n *     id: 'ce8076ee-2ed3-429d-938c-14f3218c',\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'NY' },\n *     id: '23d1a96e-3a2c-46f4-9d9e-cccd02c6',\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'LA' },\n *     id: '2bf9d627-310f-46ff-93a9-86baeae9',\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'NY' },\n *     id: 'c95e6ac0-ee9b-48de-86b2-12548fd1',\n *     type: 'tool_call'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().optional().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, { name: \"Joke\" });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   punchline: 'Because she wanted to be a first-aid kit.',\n *   rating: 5,\n *   setup: 'Why did the cat join the Red Cross?'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 75, output_tokens: 6, total_tokens: 81 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   estimatedTokenUsage: { completionTokens: 6, promptTokens: 75, totalTokens: 81 },\n *   response_id: 'a688ad65-4db2-4a7a-b6aa-124aa2410319',\n *   generationId: 'ee259727-18c5-43f7-b9bd-a2a60c0c040b',\n *   chatHistory: [\n *     {\n *       role: 'USER',\n *       message: 'Translate \"I love programming\" into French.'\n *     },\n *     { role: 'CHATBOT', message: `\"J'adore programmer.\"` }\n *   ],\n *   finishReason: 'COMPLETE',\n *   meta: {\n *     apiVersion: { version: '1' },\n *     billedUnits: { inputTokens: 9, outputTokens: 6 },\n *     tokens: { inputTokens: 75, outputTokens: 6 }\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n */\nexport class ChatCohere<\n    CallOptions extends ChatCohereCallOptions = ChatCohereCallOptions\n  >\n  extends BaseChatModel<CallOptions, AIMessageChunk>\n  implements ChatCohereInput\n{\n  static lc_name() {\n    return \"ChatCohere\";\n  }\n\n  lc_serializable = true;\n\n  client: CohereClient;\n\n  model = \"command-r-plus\";\n\n  temperature = 0.3;\n\n  streaming = false;\n\n  streamUsage: boolean = true;\n\n  constructor(fields?: ChatCohereInput) {\n    super(fields ?? {});\n\n    this.client = getCohereClient(fields);\n\n    this.model = fields?.model ?? this.model;\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.streaming = fields?.streaming ?? this.streaming;\n    this.streamUsage = fields?.streamUsage ?? this.streamUsage;\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = this.invocationParams(options);\n    return {\n      ls_provider: \"cohere\",\n      ls_model_name: this.model,\n      ls_model_type: \"chat\",\n      ls_temperature: this.temperature ?? undefined,\n      ls_max_tokens:\n        typeof params.maxTokens === \"number\" ? params.maxTokens : undefined,\n      ls_stop: Array.isArray(params.stopSequences)\n        ? (params.stopSequences as unknown as string[])\n        : undefined,\n    };\n  }\n\n  _llmType() {\n    return \"cohere\";\n  }\n\n  invocationParams(options: this[\"ParsedCallOptions\"]) {\n    if (options.tool_choice) {\n      throw new Error(\n        \"'tool_choice' call option is not supported by ChatCohere.\"\n      );\n    }\n\n    const params = {\n      model: this.model,\n      preamble: options.preamble,\n      conversationId: options.conversationId,\n      promptTruncation: options.promptTruncation,\n      connectors: options.connectors,\n      searchQueriesOnly: options.searchQueriesOnly,\n      documents: options.documents,\n      temperature: options.temperature ?? this.temperature,\n      forceSingleStep: options.forceSingleStep,\n      tools: options.tools,\n    };\n    // Filter undefined entries\n    return Object.fromEntries(\n      Object.entries(params).filter(([, value]) => value !== undefined)\n    );\n  }\n\n  override bindTools(\n    tools: ChatCohereToolType[],\n    kwargs?: Partial<CallOptions>\n  ): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions> {\n    return this.withConfig({\n      tools: _formatToolsToCohere(tools),\n      ...kwargs,\n    } as Partial<CallOptions>);\n  }\n\n  /** @ignore */\n  private _getChatRequest(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"]\n  ): Cohere.ChatRequest {\n    const params = this.invocationParams(options);\n\n    const toolResults = this._messagesToCohereToolResultsCurrChatTurn(messages);\n    const chatHistory = [];\n    let messageStr: string = \"\";\n    let tempToolResults: {\n      call: Cohere.ToolCall;\n      outputs: any;\n    }[] = [];\n\n    if (!params.forceSingleStep) {\n      for (let i = 0; i < messages.length - 1; i += 1) {\n        const message = messages[i];\n        // If there are multiple tool messages, then we need to aggregate them into one single tool message to pass into chat history\n        if (message._getType().toLowerCase() === \"tool\") {\n          tempToolResults = tempToolResults.concat(\n            this._messageToCohereToolResults(messages, i)\n          );\n\n          if (\n            i === messages.length - 1 ||\n            !(messages[i + 1]._getType().toLowerCase() === \"tool\")\n          ) {\n            const cohere_message = convertMessageToCohereMessage(\n              message,\n              tempToolResults\n            );\n            chatHistory.push(cohere_message);\n            tempToolResults = [];\n          }\n        } else {\n          chatHistory.push(convertMessageToCohereMessage(message, []));\n        }\n      }\n\n      messageStr =\n        toolResults.length > 0\n          ? \"\"\n          : messages[messages.length - 1].content.toString();\n    } else {\n      messageStr = \"\";\n\n      // if force_single_step is set to True, then message is the last human message in the conversation\n      for (let i = 0; i < messages.length - 1; i += 1) {\n        const message = messages[i];\n        if (isAIMessage(message) && message.tool_calls) {\n          continue;\n        }\n\n        // If there are multiple tool messages, then we need to aggregate them into one single tool message to pass into chat history\n        if (message._getType().toLowerCase() === \"tool\") {\n          tempToolResults = tempToolResults.concat(\n            this._messageToCohereToolResults(messages, i)\n          );\n\n          if (\n            i === messages.length - 1 ||\n            !(messages[i + 1]._getType().toLowerCase() === \"tool\")\n          ) {\n            const cohereMessage = convertMessageToCohereMessage(\n              message,\n              tempToolResults\n            );\n            chatHistory.push(cohereMessage);\n            tempToolResults = [];\n          }\n        } else {\n          chatHistory.push(convertMessageToCohereMessage(message, []));\n        }\n      }\n\n      // Add the last human message in the conversation to the message string\n      for (let i = messages.length - 1; i >= 0; i -= 1) {\n        const message = messages[i];\n        if (message._getType().toLowerCase() === \"human\" && message.content) {\n          messageStr = message.content.toString();\n          break;\n        }\n      }\n    }\n    const req: Cohere.ChatRequest = {\n      message: messageStr,\n      chatHistory,\n      toolResults: toolResults.length > 0 ? toolResults : undefined,\n      ...params,\n    };\n\n    return req;\n  }\n\n  private _getCurrChatTurnMessages(messages: BaseMessage[]): BaseMessage[] {\n    // Get the messages for the current chat turn.\n    const currentChatTurnMessages: BaseMessage[] = [];\n    for (let i = messages.length - 1; i >= 0; i -= 1) {\n      const message = messages[i];\n      currentChatTurnMessages.push(message);\n      if (message._getType().toLowerCase() === \"human\") {\n        break;\n      }\n    }\n    return currentChatTurnMessages.reverse();\n  }\n\n  private _messagesToCohereToolResultsCurrChatTurn(\n    messages: BaseMessage[]\n  ): Array<{\n    call: Cohere.ToolCall;\n    outputs: ReturnType<typeof convertToDocuments>;\n  }> {\n    /** Get tool_results from messages. */\n    const toolResults: Array<{\n      call: Cohere.ToolCall;\n      outputs: ReturnType<typeof convertToDocuments>;\n    }> = [];\n    const currChatTurnMessages = this._getCurrChatTurnMessages(messages);\n\n    for (const message of currChatTurnMessages) {\n      if (isToolMessage(message)) {\n        const toolMessage = message;\n        const previousAiMsgs = currChatTurnMessages.filter(\n          (msg) => isAIMessage(msg) && msg.tool_calls !== undefined\n        ) as AIMessage[];\n        if (previousAiMsgs.length > 0) {\n          const previousAiMsg = previousAiMsgs[previousAiMsgs.length - 1];\n          if (previousAiMsg.tool_calls) {\n            toolResults.push(\n              ...previousAiMsg.tool_calls\n                .filter(\n                  (lcToolCall) => lcToolCall.id === toolMessage.tool_call_id\n                )\n                .map((lcToolCall) => ({\n                  call: {\n                    name: lcToolCall.name,\n                    parameters: lcToolCall.args,\n                  },\n                  outputs: convertToDocuments(toolMessage.content),\n                }))\n            );\n          }\n        }\n      }\n    }\n    return toolResults;\n  }\n\n  private _messageToCohereToolResults(\n    messages: BaseMessage[],\n    toolMessageIndex: number\n  ): Array<{ call: Cohere.ToolCall; outputs: any }> {\n    /** Get tool_results from messages. */\n    const toolResults: Array<{ call: Cohere.ToolCall; outputs: any }> = [];\n    const toolMessage = messages[toolMessageIndex];\n\n    if (!isToolMessage(toolMessage)) {\n      throw new Error(\n        \"The message index does not correspond to an instance of ToolMessage\"\n      );\n    }\n\n    const messagesUntilTool = messages.slice(0, toolMessageIndex);\n    const previousAiMessage = messagesUntilTool\n      .filter((message) => isAIMessage(message) && message.tool_calls)\n      .slice(-1)[0] as AIMessage;\n\n    if (previousAiMessage.tool_calls) {\n      toolResults.push(\n        ...previousAiMessage.tool_calls\n          .filter((lcToolCall) => lcToolCall.id === toolMessage.tool_call_id)\n          .map((lcToolCall) => ({\n            call: {\n              name: lcToolCall.name,\n              parameters: lcToolCall.args,\n            },\n            outputs: convertToDocuments(toolMessage.content),\n          }))\n      );\n    }\n\n    return toolResults;\n  }\n\n  private _formatCohereToolCalls(toolCalls: Cohere.ToolCall[] | null = null): {\n    id: string;\n    function: {\n      name: string;\n      arguments: Record<string, any>;\n    };\n    type: string;\n  }[] {\n    if (!toolCalls) {\n      return [];\n    }\n\n    const formattedToolCalls = [];\n    for (const toolCall of toolCalls) {\n      formattedToolCalls.push({\n        id: uuid.v4().substring(0, 32),\n        function: {\n          name: toolCall.name,\n          arguments: toolCall.parameters, // Convert arguments to string\n        },\n        type: \"function\",\n      });\n    }\n    return formattedToolCalls;\n  }\n\n  private _convertCohereToolCallToLangchain(\n    toolCalls: Record<string, any>[]\n  ): ToolCall[] {\n    return toolCalls.map((toolCall) => ({\n      name: toolCall.function.name,\n      args: toolCall.function.arguments,\n      id: toolCall.id,\n      type: \"tool_call\",\n    }));\n  }\n\n  /** @ignore */\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    const tokenUsage: TokenUsage = {};\n    // The last message in the array is the most recent, all other messages\n    // are apart of the chat history.\n    const request = this._getChatRequest(messages, options);\n\n    // Handle streaming\n    if (this.streaming) {\n      const stream = this._streamResponseChunks(messages, options, runManager);\n      const finalChunks: Record<number, ChatGenerationChunk> = {};\n      for await (const chunk of stream) {\n        const index =\n          (chunk.generationInfo as NewTokenIndices)?.completion ?? 0;\n        if (finalChunks[index] === undefined) {\n          finalChunks[index] = chunk;\n        } else {\n          finalChunks[index] = finalChunks[index].concat(chunk);\n        }\n      }\n      const generations = Object.entries(finalChunks)\n        .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))\n        .map(([_, value]) => value);\n\n      return { generations, llmOutput: { estimatedTokenUsage: tokenUsage } };\n    }\n\n    // Not streaming, so we can just call the API once.\n    const response: Cohere.NonStreamedChatResponse =\n      await this.caller.callWithOptions(\n        { signal: options.signal },\n        async () => {\n          let response;\n          try {\n            response = await this.client.chat(request);\n          } catch (e: any) {\n            e.status = e.status ?? e.statusCode;\n            throw e;\n          }\n          return response;\n        }\n      );\n\n    if (response.meta?.tokens) {\n      const { inputTokens, outputTokens } = response.meta.tokens;\n\n      if (outputTokens) {\n        tokenUsage.completionTokens =\n          (tokenUsage.completionTokens ?? 0) + outputTokens;\n      }\n\n      if (inputTokens) {\n        tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + inputTokens;\n      }\n\n      tokenUsage.totalTokens =\n        (tokenUsage.totalTokens ?? 0) +\n        (tokenUsage.promptTokens ?? 0) +\n        (tokenUsage.completionTokens ?? 0);\n    }\n\n    const generationInfo: Record<string, unknown> = { ...response };\n    delete generationInfo.text;\n    if (response.toolCalls && response.toolCalls.length > 0) {\n      // Only populate tool_calls when 1) present on the response and\n      // 2) has one or more calls.\n      generationInfo.toolCalls = this._formatCohereToolCalls(\n        response.toolCalls\n      );\n    }\n    let toolCalls: ToolCall[] = [];\n    if (\"toolCalls\" in generationInfo) {\n      toolCalls = this._convertCohereToolCallToLangchain(\n        generationInfo.toolCalls as Record<string, any>[]\n      );\n    }\n\n    const generations: ChatGeneration[] = [\n      {\n        text: response.text,\n        message: new AIMessage({\n          content: response.text,\n          additional_kwargs: generationInfo,\n          tool_calls: toolCalls,\n          usage_metadata: {\n            input_tokens: tokenUsage.promptTokens ?? 0,\n            output_tokens: tokenUsage.completionTokens ?? 0,\n            total_tokens: tokenUsage.totalTokens ?? 0,\n          },\n        }),\n        generationInfo,\n      },\n    ];\n    return {\n      generations,\n      llmOutput: { estimatedTokenUsage: tokenUsage },\n    };\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const request = this._getChatRequest(messages, options);\n\n    // All models have a built in `this.caller` property for retries\n    const stream = await this.caller.call(async () => {\n      let stream;\n      try {\n        stream = await this.client.chatStream(request);\n      } catch (e: any) {\n        e.status = e.status ?? e.statusCode;\n        throw e;\n      }\n      return stream;\n    });\n    for await (const chunk of stream) {\n      if (chunk.eventType === \"text-generation\") {\n        yield new ChatGenerationChunk({\n          text: chunk.text,\n          message: new AIMessageChunk({\n            content: chunk.text,\n          }),\n        });\n        await runManager?.handleLLMNewToken(chunk.text);\n      } else if (chunk.eventType !== \"stream-end\") {\n        // Used for when the user uses their RAG/Search/other API\n        // and the stream takes more actions then just text generation.\n        yield new ChatGenerationChunk({\n          text: \"\",\n          message: new AIMessageChunk({\n            content: \"\",\n            additional_kwargs: {\n              ...chunk,\n            },\n          }),\n          generationInfo: {\n            ...chunk,\n          },\n        });\n      } else if (\n        chunk.eventType === \"stream-end\" &&\n        (this.streamUsage || options.streamUsage)\n      ) {\n        // stream-end events contain the final token count\n        const input_tokens = chunk.response.meta?.tokens?.inputTokens ?? 0;\n        const output_tokens = chunk.response.meta?.tokens?.outputTokens ?? 0;\n        const chunkGenerationInfo: Record<string, any> = {\n          ...chunk.response,\n        };\n\n        if (chunk.response.toolCalls && chunk.response.toolCalls.length > 0) {\n          // Only populate tool_calls when 1) present on the response and\n          // 2) has one or more calls.\n          chunkGenerationInfo.toolCalls = this._formatCohereToolCalls(\n            chunk.response.toolCalls\n          );\n        }\n\n        let toolCallChunks: ToolCallChunk[] = [];\n        const toolCalls = chunkGenerationInfo.toolCalls ?? [];\n\n        if (toolCalls.length > 0) {\n          toolCallChunks = toolCalls.map((toolCall: any) => ({\n            name: toolCall.function.name,\n            args: toolCall.function.arguments,\n            id: toolCall.id,\n            index: toolCall.index,\n            type: \"tool_call_chunk\",\n          }));\n        }\n\n        yield new ChatGenerationChunk({\n          text: \"\",\n          message: new AIMessageChunk({\n            content: \"\",\n            additional_kwargs: {\n              eventType: \"stream-end\",\n            },\n            tool_call_chunks: toolCallChunks,\n            usage_metadata: {\n              input_tokens,\n              output_tokens,\n              total_tokens: input_tokens + output_tokens,\n            },\n          }),\n          generationInfo: {\n            eventType: \"stream-end\",\n            ...chunkGenerationInfo,\n          },\n        });\n      }\n    }\n  }\n\n  _combineLLMOutput(...llmOutputs: CohereLLMOutput[]): CohereLLMOutput {\n    return llmOutputs.reduce<{\n      [key in keyof CohereLLMOutput]: Required<CohereLLMOutput[key]>;\n    }>(\n      (acc, llmOutput) => {\n        if (llmOutput && llmOutput.estimatedTokenUsage) {\n          let completionTokens = acc.estimatedTokenUsage?.completionTokens ?? 0;\n          let promptTokens = acc.estimatedTokenUsage?.promptTokens ?? 0;\n          let totalTokens = acc.estimatedTokenUsage?.totalTokens ?? 0;\n\n          completionTokens +=\n            llmOutput.estimatedTokenUsage.completionTokens ?? 0;\n          promptTokens += llmOutput.estimatedTokenUsage.promptTokens ?? 0;\n          totalTokens += llmOutput.estimatedTokenUsage.totalTokens ?? 0;\n\n          acc.estimatedTokenUsage = {\n            completionTokens,\n            promptTokens,\n            totalTokens,\n          };\n        }\n        return acc;\n      },\n      {\n        estimatedTokenUsage: {\n          completionTokens: 0,\n          promptTokens: 0,\n          totalTokens: 0,\n        },\n      }\n    );\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      apiKey: \"COHERE_API_KEY\",\n      api_key: \"COHERE_API_KEY\",\n    };\n  }\n\n  get lc_aliases(): { [key: string]: string } | undefined {\n    return {\n      apiKey: \"cohere_api_key\",\n      api_key: \"cohere_api_key\",\n    };\n  }\n}\n\ninterface CohereLLMOutput {\n  estimatedTokenUsage?: TokenUsage;\n}\n"],"mappings":";;;;;;;;;;;;AAgGA,SAAS,mBACPA,cAC4B;;CAE5B,MAAMC,YAAwC,CAAE;CAChD,IAAIC,mBAA+C,CAAE;AAErD,KAAI,OAAO,iBAAiB,UAE1B,mBAAmB,CAAC,EAAE,QAAQ,aAAc,CAAC;UAG7C,wBAAwB,OACvB,OAAO,iBAAiB,YACvB,iBAAiB,QACjB,CAAC,MAAM,QAAQ,aAAa,EAG9B,mBAAmB,CAAC,YAAa;UACxB,CAAC,MAAM,QAAQ,aAAa,EAErC,mBAAmB,CAAC,EAAE,QAAQ,aAAc,CAAC;AAG/C,MAAK,IAAI,OAAO,kBAAkB;AAEhC,MAAI,EAAE,eAAe,SAAS,OAAO,QAAQ,YAAY,QAAQ,OAE/D,MAAM,EAAE,QAAQ,IAAK;EAEvB,UAAU,KAAK,IAAI;CACpB;AAED,QAAO;AACR;AAED,SAAS,8BACPC,SACAC,aACgB;CAChB,MAAM,UAAU,CAACC,SAAsB;AACrC,UAAQ,MAAR;GACE,KAAK,SACH,QAAO;GACT,KAAK,QACH,QAAO;GACT,KAAK,KACH,QAAO;GACT,KAAK,OACH,QAAO;GACT,QACE,OAAM,IAAI,MACR,CAAC,uBAAuB,EAAE,KAAK,kDAAkD,CAAC;EAEvF;CACF;CAED,MAAM,aAAa,CAACC,YAAoC;AACtD,MAAI,OAAO,YAAY,SACrB,QAAO;AAET,QAAM,IAAI,MACR,CAAC,gEAAgE,EAAE,KAAK,UACtE,SACA,MACA,EACD,EAAE;CAEN;CAED,MAAM,cAAc,CAACH,cAA4C;AAC/D,iDAAgBI,UAAQ,IAAIA,UAAQ,WAClC,QAAOA,UAAQ,WAAW,IAAI,CAAC,cAAc;GAC3C,MAAM,SAAS;GACf,YAAY,SAAS;EACtB,GAAE;AAEL,SAAO,CAAE;CACV;AACD,KAAI,QAAQ,UAAU,CAAC,aAAa,KAAK,KACvC,QAAO;EACL,MAAM,QAAQ,QAAQ,UAAU,CAAC;EACjC,SAAS,WAAW,QAAQ,QAAQ;EACpC,WAAW,YAAY,QAAQ;CAChC;UACQ,QAAQ,UAAU,CAAC,aAAa,KAAK,OAC9C,QAAO;EACL,MAAM,QAAQ,QAAQ,UAAU,CAAC;EACjC,SAAS,WAAW,QAAQ,QAAQ;EACpC;CACD;UAED,QAAQ,UAAU,CAAC,aAAa,KAAK,WACrC,QAAQ,UAAU,CAAC,aAAa,KAAK,SAErC,QAAO;EACL,MAAM,QAAQ,QAAQ,UAAU,CAAC;EACjC,SAAS,WAAW,QAAQ,QAAQ;CACrC;KAED,OAAM,IAAI,MACR;AAGL;AAED,SAAS,aAAaC,MAAgC;AACpD,QACE,UAAU,QAAQ,iBAAiB,QAAQ,0BAA0B;AAExE;AAED,SAAS,cAAcL,SAA8C;AACnE,QAAO,QAAQ,UAAU,KAAK;AAC/B;AAED,SAAS,+BAA+BM,YAAiC;CACvE,MAAM,iCACJ,gBAAgB,aAAa,WAAW,aAAa,CAAE;CACzD,IAAI,+BACF,cAAc,aAAa,WAAW,WAAW,CAAE;CAErD,MAAMC,4BAAiD,CAAE;CAGzD,OAAO,KAAK,+BAA+B,CAAC,QAAQ,CAAC,iBAAiB;EAEpE,0BAA0B,gBACxB,+BAA+B;AAEjC,MAAI,iCAAiC,QACnC,+BAA+B,CAAE;EAEnC,0BAA0B,cAAc,WACtC,6BAA6B,SAAS,aAAa;CACtD,EAAC;AACF,QAAO;AACR;AAED,SAAS,qBACPC,OAC2B;AAC3B,KAAI,CAAC,MACH,QAAO;UACE,MAAM,MAAM,aAAa,CAClC,QAAO;UACE,MAAM,MAAMC,mDAAa,CAClC,QAAO,MAAM,IAAI,CAAC,SAAS;AACzB,SAAO;GACL,MAAM,KAAK,SAAS;GACpB,aAAa,KAAK,SAAS,eAAe;GAC1C,sBAAsB,+BACpB,KAAK,SAAS,WACf;EACF;CACF,EAAC;UACO,MAAM,MAAMC,wDAAgB,CACrC,QAAO,MAAM,IAAI,CAAC,SAAS;EACzB,MAAM,mFAAiD,KAAK,OAAO,wDAClD,KAAK,OAAO,GACzB,KAAK;AACT,SAAO;GACL,MAAM,KAAK;GACX,aAAa,KAAK,eAAe;GACjC,sBAAsB,+BACpB,4BACD;EACF;CACF,EAAC;KAEF,OAAM,IAAI,MACR,CAAC,yDAAyD,CAAC;AAGhE;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAwbD,IAAa,aAAb,cAGUC,2DAEV;CACE,OAAO,UAAU;AACf,SAAO;CACR;CAED,kBAAkB;CAElB;CAEA,QAAQ;CAER,cAAc;CAEd,YAAY;CAEZ,cAAuB;CAEvB,YAAYC,QAA0B;EACpC,MAAM,UAAU,CAAE,EAAC;EAEnB,KAAK,SAASC,+BAAgB,OAAO;EAErC,KAAK,QAAQ,QAAQ,SAAS,KAAK;EACnC,KAAK,cAAc,QAAQ,eAAe,KAAK;EAC/C,KAAK,YAAY,QAAQ,aAAa,KAAK;EAC3C,KAAK,cAAc,QAAQ,eAAe,KAAK;CAChD;CAED,YAAYC,SAAqD;EAC/D,MAAM,SAAS,KAAK,iBAAiB,QAAQ;AAC7C,SAAO;GACL,aAAa;GACb,eAAe,KAAK;GACpB,eAAe;GACf,gBAAgB,KAAK,eAAe;GACpC,eACE,OAAO,OAAO,cAAc,WAAW,OAAO,YAAY;GAC5D,SAAS,MAAM,QAAQ,OAAO,cAAc,GACvC,OAAO,gBACR;EACL;CACF;CAED,WAAW;AACT,SAAO;CACR;CAED,iBAAiBA,SAAoC;AACnD,MAAI,QAAQ,YACV,OAAM,IAAI,MACR;EAIJ,MAAM,SAAS;GACb,OAAO,KAAK;GACZ,UAAU,QAAQ;GAClB,gBAAgB,QAAQ;GACxB,kBAAkB,QAAQ;GAC1B,YAAY,QAAQ;GACpB,mBAAmB,QAAQ;GAC3B,WAAW,QAAQ;GACnB,aAAa,QAAQ,eAAe,KAAK;GACzC,iBAAiB,QAAQ;GACzB,OAAO,QAAQ;EAChB;AAED,SAAO,OAAO,YACZ,OAAO,QAAQ,OAAO,CAAC,OAAO,CAAC,GAAG,MAAM,KAAK,UAAU,OAAU,CAClE;CACF;CAED,AAAS,UACPC,OACAC,QAC+D;AAC/D,SAAO,KAAK,WAAW;GACrB,OAAO,qBAAqB,MAAM;GAClC,GAAG;EACJ,EAAyB;CAC3B;;CAGD,AAAQ,gBACNC,UACAH,SACoB;EACpB,MAAM,SAAS,KAAK,iBAAiB,QAAQ;EAE7C,MAAM,cAAc,KAAK,yCAAyC,SAAS;EAC3E,MAAM,cAAc,CAAE;EACtB,IAAII,aAAqB;EACzB,IAAIC,kBAGE,CAAE;AAER,MAAI,CAAC,OAAO,iBAAiB;AAC3B,QAAK,IAAI,IAAI,GAAG,IAAI,SAAS,SAAS,GAAG,KAAK,GAAG;IAC/C,MAAM,UAAU,SAAS;AAEzB,QAAI,QAAQ,UAAU,CAAC,aAAa,KAAK,QAAQ;KAC/C,kBAAkB,gBAAgB,OAChC,KAAK,4BAA4B,UAAU,EAAE,CAC9C;AAED,SACE,MAAM,SAAS,SAAS,KACxB,EAAE,SAAS,IAAI,GAAG,UAAU,CAAC,aAAa,KAAK,SAC/C;MACA,MAAM,iBAAiB,8BACrB,SACA,gBACD;MACD,YAAY,KAAK,eAAe;MAChC,kBAAkB,CAAE;KACrB;IACF,OACC,YAAY,KAAK,8BAA8B,SAAS,CAAE,EAAC,CAAC;GAE/D;GAED,aACE,YAAY,SAAS,IACjB,KACA,SAAS,SAAS,SAAS,GAAG,QAAQ,UAAU;EACvD,OAAM;GACL,aAAa;AAGb,QAAK,IAAI,IAAI,GAAG,IAAI,SAAS,SAAS,GAAG,KAAK,GAAG;IAC/C,MAAM,UAAU,SAAS;AACzB,mDAAgB,QAAQ,IAAI,QAAQ,WAClC;AAIF,QAAI,QAAQ,UAAU,CAAC,aAAa,KAAK,QAAQ;KAC/C,kBAAkB,gBAAgB,OAChC,KAAK,4BAA4B,UAAU,EAAE,CAC9C;AAED,SACE,MAAM,SAAS,SAAS,KACxB,EAAE,SAAS,IAAI,GAAG,UAAU,CAAC,aAAa,KAAK,SAC/C;MACA,MAAM,gBAAgB,8BACpB,SACA,gBACD;MACD,YAAY,KAAK,cAAc;MAC/B,kBAAkB,CAAE;KACrB;IACF,OACC,YAAY,KAAK,8BAA8B,SAAS,CAAE,EAAC,CAAC;GAE/D;AAGD,QAAK,IAAI,IAAI,SAAS,SAAS,GAAG,KAAK,GAAG,KAAK,GAAG;IAChD,MAAM,UAAU,SAAS;AACzB,QAAI,QAAQ,UAAU,CAAC,aAAa,KAAK,WAAW,QAAQ,SAAS;KACnE,aAAa,QAAQ,QAAQ,UAAU;AACvC;IACD;GACF;EACF;EACD,MAAMC,MAA0B;GAC9B,SAAS;GACT;GACA,aAAa,YAAY,SAAS,IAAI,cAAc;GACpD,GAAG;EACJ;AAED,SAAO;CACR;CAED,AAAQ,yBAAyBH,UAAwC;EAEvE,MAAMI,0BAAyC,CAAE;AACjD,OAAK,IAAI,IAAI,SAAS,SAAS,GAAG,KAAK,GAAG,KAAK,GAAG;GAChD,MAAM,UAAU,SAAS;GACzB,wBAAwB,KAAK,QAAQ;AACrC,OAAI,QAAQ,UAAU,CAAC,aAAa,KAAK,QACvC;EAEH;AACD,SAAO,wBAAwB,SAAS;CACzC;CAED,AAAQ,yCACNJ,UAIC;;EAED,MAAMK,cAGD,CAAE;EACP,MAAM,uBAAuB,KAAK,yBAAyB,SAAS;AAEpE,OAAK,MAAM,WAAW,qBACpB,KAAI,cAAc,QAAQ,EAAE;GAC1B,MAAM,cAAc;GACpB,MAAM,iBAAiB,qBAAqB,OAC1C,CAAC,mDAAoB,IAAI,IAAI,IAAI,eAAe,OACjD;AACD,OAAI,eAAe,SAAS,GAAG;IAC7B,MAAM,gBAAgB,eAAe,eAAe,SAAS;AAC7D,QAAI,cAAc,YAChB,YAAY,KACV,GAAG,cAAc,WACd,OACC,CAAC,eAAe,WAAW,OAAO,YAAY,aAC/C,CACA,IAAI,CAAC,gBAAgB;KACpB,MAAM;MACJ,MAAM,WAAW;MACjB,YAAY,WAAW;KACxB;KACD,SAAS,mBAAmB,YAAY,QAAQ;IACjD,GAAE,CACN;GAEJ;EACF;AAEH,SAAO;CACR;CAED,AAAQ,4BACNL,UACAM,kBACgD;;EAEhD,MAAMC,cAA8D,CAAE;EACtE,MAAM,cAAc,SAAS;AAE7B,MAAI,CAAC,cAAc,YAAY,CAC7B,OAAM,IAAI,MACR;EAIJ,MAAM,oBAAoB,SAAS,MAAM,GAAG,iBAAiB;EAC7D,MAAM,oBAAoB,kBACvB,OAAO,CAAC,uDAAwB,QAAQ,IAAI,QAAQ,WAAW,CAC/D,MAAM,GAAG,CAAC;AAEb,MAAI,kBAAkB,YACpB,YAAY,KACV,GAAG,kBAAkB,WAClB,OAAO,CAAC,eAAe,WAAW,OAAO,YAAY,aAAa,CAClE,IAAI,CAAC,gBAAgB;GACpB,MAAM;IACJ,MAAM,WAAW;IACjB,YAAY,WAAW;GACxB;GACD,SAAS,mBAAmB,YAAY,QAAQ;EACjD,GAAE,CACN;AAGH,SAAO;CACR;CAED,AAAQ,uBAAuBC,YAAsC,MAOjE;AACF,MAAI,CAAC,UACH,QAAO,CAAE;EAGX,MAAM,qBAAqB,CAAE;AAC7B,OAAK,MAAM,YAAY,WACrB,mBAAmB,KAAK;GACtB,IAAI,KAAK,IAAI,CAAC,UAAU,GAAG,GAAG;GAC9B,UAAU;IACR,MAAM,SAAS;IACf,WAAW,SAAS;GACrB;GACD,MAAM;EACP,EAAC;AAEJ,SAAO;CACR;CAED,AAAQ,kCACNC,WACY;AACZ,SAAO,UAAU,IAAI,CAAC,cAAc;GAClC,MAAM,SAAS,SAAS;GACxB,MAAM,SAAS,SAAS;GACxB,IAAI,SAAS;GACb,MAAM;EACP,GAAE;CACJ;;CAGD,MAAM,UACJT,UACAH,SACAa,YACqB;EACrB,MAAMC,aAAyB,CAAE;EAGjC,MAAM,UAAU,KAAK,gBAAgB,UAAU,QAAQ;AAGvD,MAAI,KAAK,WAAW;GAClB,MAAM,SAAS,KAAK,sBAAsB,UAAU,SAAS,WAAW;GACxE,MAAMC,cAAmD,CAAE;AAC3D,cAAW,MAAM,SAAS,QAAQ;IAChC,MAAM,QACH,MAAM,gBAAoC,cAAc;AAC3D,QAAI,YAAY,WAAW,QACzB,YAAY,SAAS;SAErB,YAAY,SAAS,YAAY,OAAO,OAAO,MAAM;GAExD;GACD,MAAMC,gBAAc,OAAO,QAAQ,YAAY,CAC5C,KAAK,CAAC,CAAC,KAAK,EAAE,CAAC,KAAK,KAAK,SAAS,MAAM,GAAG,GAAG,SAAS,MAAM,GAAG,CAAC,CACjE,IAAI,CAAC,CAAC,GAAG,MAAM,KAAK,MAAM;AAE7B,UAAO;IAAE;IAAa,WAAW,EAAE,qBAAqB,WAAY;GAAE;EACvE;EAGD,MAAMC,WACJ,MAAM,KAAK,OAAO,gBAChB,EAAE,QAAQ,QAAQ,OAAQ,GAC1B,YAAY;GACV,IAAIC;AACJ,OAAI;IACFA,aAAW,MAAM,KAAK,OAAO,KAAK,QAAQ;GAC3C,SAAQC,GAAQ;IACf,EAAE,SAAS,EAAE,UAAU,EAAE;AACzB,UAAM;GACP;AACD,UAAOD;EACR,EACF;AAEH,MAAI,SAAS,MAAM,QAAQ;GACzB,MAAM,EAAE,aAAa,cAAc,GAAG,SAAS,KAAK;AAEpD,OAAI,cACF,WAAW,oBACR,WAAW,oBAAoB,KAAK;AAGzC,OAAI,aACF,WAAW,gBAAgB,WAAW,gBAAgB,KAAK;GAG7D,WAAW,eACR,WAAW,eAAe,MAC1B,WAAW,gBAAgB,MAC3B,WAAW,oBAAoB;EACnC;EAED,MAAME,iBAA0C,EAAE,GAAG,SAAU;EAC/D,OAAO,eAAe;AACtB,MAAI,SAAS,aAAa,SAAS,UAAU,SAAS,GAGpD,eAAe,YAAY,KAAK,uBAC9B,SAAS,UACV;EAEH,IAAIC,YAAwB,CAAE;AAC9B,MAAI,eAAe,gBACjB,YAAY,KAAK,kCACf,eAAe,UAChB;EAGH,MAAMC,cAAgC,CACpC;GACE,MAAM,SAAS;GACf,SAAS,IAAIC,oCAAU;IACrB,SAAS,SAAS;IAClB,mBAAmB;IACnB,YAAY;IACZ,gBAAgB;KACd,cAAc,WAAW,gBAAgB;KACzC,eAAe,WAAW,oBAAoB;KAC9C,cAAc,WAAW,eAAe;IACzC;GACF;GACD;EACD,CACF;AACD,SAAO;GACL;GACA,WAAW,EAAE,qBAAqB,WAAY;EAC/C;CACF;CAED,OAAO,sBACLpB,UACAH,SACAa,YACqC;EACrC,MAAM,UAAU,KAAK,gBAAgB,UAAU,QAAQ;EAGvD,MAAM,SAAS,MAAM,KAAK,OAAO,KAAK,YAAY;GAChD,IAAIW;AACJ,OAAI;IACFA,WAAS,MAAM,KAAK,OAAO,WAAW,QAAQ;GAC/C,SAAQL,GAAQ;IACf,EAAE,SAAS,EAAE,UAAU,EAAE;AACzB,UAAM;GACP;AACD,UAAOK;EACR,EAAC;AACF,aAAW,MAAM,SAAS,OACxB,KAAI,MAAM,cAAc,mBAAmB;GACzC,MAAM,IAAIC,6CAAoB;IAC5B,MAAM,MAAM;IACZ,SAAS,IAAIC,yCAAe,EAC1B,SAAS,MAAM,KAChB;GACF;GACD,MAAM,YAAY,kBAAkB,MAAM,KAAK;EAChD,WAAU,MAAM,cAAc,cAG7B,MAAM,IAAID,6CAAoB;GAC5B,MAAM;GACN,SAAS,IAAIC,yCAAe;IAC1B,SAAS;IACT,mBAAmB,EACjB,GAAG,MACJ;GACF;GACD,gBAAgB,EACd,GAAG,MACJ;EACF;WAED,MAAM,cAAc,iBACnB,KAAK,eAAe,QAAQ,cAC7B;GAEA,MAAM,eAAe,MAAM,SAAS,MAAM,QAAQ,eAAe;GACjE,MAAM,gBAAgB,MAAM,SAAS,MAAM,QAAQ,gBAAgB;GACnE,MAAMC,sBAA2C,EAC/C,GAAG,MAAM,SACV;AAED,OAAI,MAAM,SAAS,aAAa,MAAM,SAAS,UAAU,SAAS,GAGhE,oBAAoB,YAAY,KAAK,uBACnC,MAAM,SAAS,UAChB;GAGH,IAAIC,iBAAkC,CAAE;GACxC,MAAM,YAAY,oBAAoB,aAAa,CAAE;AAErD,OAAI,UAAU,SAAS,GACrB,iBAAiB,UAAU,IAAI,CAACC,cAAmB;IACjD,MAAM,SAAS,SAAS;IACxB,MAAM,SAAS,SAAS;IACxB,IAAI,SAAS;IACb,OAAO,SAAS;IAChB,MAAM;GACP,GAAE;GAGL,MAAM,IAAIJ,6CAAoB;IAC5B,MAAM;IACN,SAAS,IAAIC,yCAAe;KAC1B,SAAS;KACT,mBAAmB,EACjB,WAAW,aACZ;KACD,kBAAkB;KAClB,gBAAgB;MACd;MACA;MACA,cAAc,eAAe;KAC9B;IACF;IACD,gBAAgB;KACd,WAAW;KACX,GAAG;IACJ;GACF;EACF;CAEJ;CAED,kBAAkB,GAAG,YAAgD;AACnE,SAAO,WAAW,OAGhB,CAAC,KAAK,cAAc;AAClB,OAAI,aAAa,UAAU,qBAAqB;IAC9C,IAAI,mBAAmB,IAAI,qBAAqB,oBAAoB;IACpE,IAAI,eAAe,IAAI,qBAAqB,gBAAgB;IAC5D,IAAI,cAAc,IAAI,qBAAqB,eAAe;IAE1D,oBACE,UAAU,oBAAoB,oBAAoB;IACpD,gBAAgB,UAAU,oBAAoB,gBAAgB;IAC9D,eAAe,UAAU,oBAAoB,eAAe;IAE5D,IAAI,sBAAsB;KACxB;KACA;KACA;IACD;GACF;AACD,UAAO;EACR,GACD,EACE,qBAAqB;GACnB,kBAAkB;GAClB,cAAc;GACd,aAAa;EACd,EACF,EACF;CACF;CAED,IAAI,aAAoD;AACtD,SAAO;GACL,QAAQ;GACR,SAAS;EACV;CACF;CAED,IAAI,aAAoD;AACtD,SAAO;GACL,QAAQ;GACR,SAAS;EACV;CACF;AACF"}